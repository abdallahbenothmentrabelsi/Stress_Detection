{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkNelz8KLvuy"
      },
      "outputs": [],
      "source": [
        "!pip install mediapipe\n",
        "!pip install dlib\n",
        "!pip install numpy==1.19.3\n",
        "!pip install opencv-contrib-python\n",
        "!pip install msvc-runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bEglyCO0MCL3"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import os \n",
        "from google.colab.patches import cv2_imshow\n",
        " \n",
        "# image = cv2.imread(\"/content/person.jfif\")\n",
        "# video = cv2.VideoCapture(r'/content/video.mp4') \n",
        "# face mesh \n",
        "mp_face_mesh = mp.solutions.face_mesh.FaceMesh()# instansiate the Facemesh function\n",
        "cap = cv2.VideoCapture(r'/content/production ID_3762907.mp4') \n",
        "\n",
        "while True:\n",
        "  ret,image = cap.read()\n",
        "  if ret is not True:\n",
        "    break\n",
        "\n",
        "  height, width, _ = image.shape\n",
        "  rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  #converting the image to RGB \n",
        "  #Facial landmarks\n",
        "  result = mp_face_mesh.process(rgb_image)\n",
        "  for facial_landmarks in result.multi_face_landmarks:\n",
        "    for i in range(0, 468):\n",
        "      pt1 = facial_landmarks.landmark[i]\n",
        "      # print (pt1)\n",
        "      x = int(pt1.x * width )\n",
        "      y = int(pt1.y * height)\n",
        "      cv2.circle(image, (x,y),2, (100, 100, 0),-1)\n",
        "  cv2_imshow(  image)\n",
        "  cv2.waitKey(1)\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "cap.release()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlGmaHp8nmZF"
      },
      "source": [
        "eye Blink detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7onUCINqVNvv",
        "outputId": "120d8158-4304-4440-d691-d8bb42240b27"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "import mediapipe as mp\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "# color identification\n",
        "# values =(blue, green, red) opencv accepts BGR values not RGB\n",
        "BLACK = (0,0,0)\n",
        "WHITE = (255,255,255)\n",
        "BLUE = (255,0,0)\n",
        "RED = (0,0,255)\n",
        "CYAN = (255,255,0)\n",
        "YELLOW =(0,255,255)\n",
        "MAGENTA = (255,0,255)\n",
        "GRAY = (128,128,128)\n",
        "GREEN = (0,255,0)\n",
        "PURPLE = (128,0,128)\n",
        "ORANGE = (0,165,255)\n",
        "PINK = (147,20,255)\n",
        "points_list =[(200, 300), (150, 150), (400, 200)]\n",
        "#this function for drawing rectanglers\n",
        "def drawColor(img, colors):\n",
        "    x, y = 0,10\n",
        "    w, h = 20, 30    \n",
        "    for color in colors:\n",
        "        x += w+5 \n",
        "        # y += 10 \n",
        "        cv.rectangle(img, (x-6, y-5 ), (x+w+5, y+h+5), (10, 50, 10), -1)\n",
        "        cv.rectangle(img, (x, y ), (x+w, y+h), color, -1)\n",
        "# this function for \n",
        "def colorBackgroundText(img, text, font, fontScale, textPos, textThickness=1,textColor=(0,255,0), bgColor=(0,0,0), pad_x=3, pad_y=3):\n",
        "\n",
        "    (t_w, t_h), _= cv.getTextSize(text, font, fontScale, textThickness) # getting the text size\n",
        "    x, y = textPos\n",
        "    cv.rectangle(img, (x-pad_x, y+ pad_y), (x+t_w+pad_x, y-t_h-pad_y), bgColor,-1) # draw rectangle \n",
        "    cv.putText(img,text, textPos,font, fontScale, textColor,textThickness ) # draw in text\n",
        "\n",
        "    return img\n",
        "\n",
        "def textWithBackground(img, text, font, fontScale, textPos, textThickness=1,textColor=(0,255,0), bgColor=(0,0,0), pad_x=3, pad_y=3, bgOpacity=0.5):\n",
        "   \n",
        "    (t_w, t_h), _= cv.getTextSize(text, font, fontScale, textThickness) # getting the text size\n",
        "    x, y = textPos\n",
        "    overlay = img.copy() # coping the image\n",
        "    cv.rectangle(overlay, (x-pad_x, y+ pad_y), (x+t_w+pad_x, y-t_h-pad_y), bgColor,-1) # draw rectangle \n",
        "    new_img = cv.addWeighted(overlay, bgOpacity, img, 1 - bgOpacity, 0) # overlaying the rectangle on the image.\n",
        "    cv.putText(new_img,text, textPos,font, fontScale, textColor,textThickness ) # draw in text\n",
        "    img = new_img\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def textBlurBackground(img, text, font, fontScale, textPos, textThickness=1,textColor=(0,255,0),kneral=(33,33) , pad_x=3, pad_y=3):\n",
        "     \n",
        "    \n",
        "    (t_w, t_h), _= cv.getTextSize(text, font, fontScale, textThickness) # getting the text size\n",
        "    x, y = textPos\n",
        "    blur_roi = img[y-pad_y-t_h: y+pad_y, x-pad_x:x+t_w+pad_x] # croping Text Background\n",
        "    img[y-pad_y-t_h: y+pad_y, x-pad_x:x+t_w+pad_x]=cv.blur(blur_roi, kneral)  # merging the blured background to img\n",
        "    cv.putText(img,text, textPos,font, fontScale, textColor,textThickness )          \n",
        "    return img\n",
        "\n",
        "def fillPolyTrans(img, points, color, opacity):\n",
        "    \n",
        "    list_to_np_array = np.array(points, dtype=np.int32)\n",
        "    overlay = img.copy()  # coping the image\n",
        "    cv.fillPoly(overlay,[list_to_np_array], color )\n",
        "    new_img = cv.addWeighted(overlay, opacity, img, 1 - opacity, 0)\n",
        "    img = new_img\n",
        "    cv.polylines(img, [list_to_np_array], True, color,1, cv.LINE_AA)\n",
        "    return img\n",
        "\n",
        "def rectTrans(img, pt1, pt2, color, thickness, opacity):\n",
        "    \n",
        "    overlay = img.copy()\n",
        "    cv.rectangle(overlay, pt1, pt2, color, thickness)\n",
        "    new_img = cv.addWeighted(overlay, opacity, img, 1 - opacity, 0) # overlaying the rectangle on the image.\n",
        "    img = new_img\n",
        "\n",
        "    return img\n",
        "\n",
        "def main():\n",
        "    cap = cv.VideoCapture('Girl.mp4')\n",
        "    counter =0\n",
        "    while True:\n",
        "        success, img = cap.read()\n",
        "        img=rectTrans(img, pt1=(30, 320), pt2=(160, 260), color=(0,255,255),thickness=-1, opacity=0.6)\n",
        "        img =fillPolyTrans(img=img, points=points_list, color=(0,255,0), opacity=.5)\n",
        "        drawColor(img, [BLACK,WHITE ,BLUE,RED,CYAN,YELLOW,MAGENTA,GRAY ,GREEN,PURPLE,ORANGE,PINK])\n",
        "        textBlurBackground(img, 'Blured Background Text', cv.FONT_HERSHEY_COMPLEX, 0.8, (60, 140),2, YELLOW, (71,71), 13, 13)\n",
        "        img=textWithBackground(img, 'Colored Background Texts', cv.FONT_HERSHEY_SIMPLEX, 0.8, (60,80), textThickness=2, bgColor=GREEN, textColor=BLACK, bgOpacity=0.7, pad_x=6, pad_y=6)\n",
        "        imgGray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
        "        # cv.imwrite('color_image.png', img)\n",
        "        counter +=1\n",
        "        cv.imshow('img', img)\n",
        "        cv.imwrite(f'image/image_{counter}.png', img)\n",
        "        if cv.waitKey(1) ==ord('q'):\n",
        "            break\n",
        " \n",
        "# variables \n",
        "frame_counter =0\n",
        "CEF_COUNTER =0\n",
        "TOTAL_BLINKS =0\n",
        "# constants\n",
        "CLOSED_EYES_FRAME =3\n",
        "FONTS =cv.FONT_HERSHEY_COMPLEX\n",
        "\n",
        "# face bounder indices \n",
        "FACE_OVAL=[ 10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288, 397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136, 172, 58, 132, 93, 234, 127, 162, 21, 54, 103,67, 109]\n",
        "\n",
        "# lips indices for Landmarks\n",
        "LIPS=[ 61, 146, 91, 181, 84, 17, 314, 405, 321, 375,291, 308, 324, 318, 402, 317, 14, 87, 178, 88, 95,185, 40, 39, 37,0 ,267 ,269 ,270 ,409, 415, 310, 311, 312, 13, 82, 81, 42, 183, 78 ]\n",
        "LOWER_LIPS =[61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291, 308, 324, 318, 402, 317, 14, 87, 178, 88, 95]\n",
        "UPPER_LIPS=[ 185, 40, 39, 37,0 ,267 ,269 ,270 ,409, 415, 310, 311, 312, 13, 82, 81, 42, 183, 78] \n",
        "# Left eyes indices \n",
        "LEFT_EYE =[ 362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385,384, 398 ]\n",
        "LEFT_EYEBROW =[ 336, 296, 334, 293, 300, 276, 283, 282, 295, 285 ]\n",
        "\n",
        "# right eyes indices\n",
        "RIGHT_EYE=[ 33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161 , 246 ]  \n",
        "RIGHT_EYEBROW=[ 70, 63, 105, 66, 107, 55, 65, 52, 53, 46 ]\n",
        "\n",
        "map_face_mesh = mp.solutions.face_mesh\n",
        "# camera object \n",
        "camera = cv.VideoCapture(r'/content/pexels-tim-douglas-6565832 (1).mp4') \n",
        "# landmark detection function \n",
        "def landmarksDetection(img, results, draw=False):\n",
        "    img_height, img_width= img.shape[:2]\n",
        "    # list[(x,y), (x,y)....]\n",
        "    mesh_coord = [(int(point.x * img_width), int(point.y * img_height)) for point in results.multi_face_landmarks[0].landmark]\n",
        "    if draw :\n",
        "        [cv.circle(img, p, 2, (0,255,0), -1) for p in mesh_coord]\n",
        "\n",
        "    # returning the list of tuples for each landmarks \n",
        "    return mesh_coord\n",
        "\n",
        "# Euclaidean distance \n",
        "def euclaideanDistance(point, point1):\n",
        "    x, y = point\n",
        "    x1, y1 = point1\n",
        "    distance = math.sqrt((x1 - x)**2 + (y1 - y)**2)\n",
        "    return distance\n",
        "\n",
        "# Blinking Ratio\n",
        "def EyebrowRatio(img, landmarks, right_indices, left_indices):\n",
        "    # Right eyes \n",
        "    # horizontal line \n",
        "    rh_right = landmarks[right_indices[0]]\n",
        "    rh_left = landmarks[right_indices[8]]\n",
        "    # vertical line \n",
        "    rv_top = landmarks[right_indices[12]]\n",
        "    rv_bottom = landmarks[right_indices[4]]\n",
        "     \n",
        "    # LEFT_EYE \n",
        "    # horizontal line \n",
        "    # lh_right = landmarks[left_indices[0]]\n",
        "    # lh_left = landmarks[left_indices[8]]\n",
        "\n",
        "    # vertical line \n",
        "    lv_top = landmarks[left_indices[12]]\n",
        "    lv_bottom = landmarks[left_indices[4]]\n",
        "\n",
        "    rhDistance = euclaideanDistance(rh_right, rh_left)\n",
        "    rvDistance = euclaideanDistance(rv_top, rv_bottom)\n",
        "\n",
        "    lvDistance = euclaideanDistance(lv_top, lv_bottom)\n",
        "    lhDistance = euclaideanDistance(lh_right, lh_left)\n",
        "\n",
        "    reRatio = rhDistance/rvDistance\n",
        "    leRatio = lhDistance/lvDistance\n",
        "\n",
        "    ratio = (reRatio+leRatio)/2\n",
        "    return ratio \n",
        "\n",
        "def blinkRatio(img, landmarks, right_indices, left_indices):\n",
        "    # Right eyes \n",
        "    # horizontal line \n",
        "    rh_right = landmarks[right_indices[0]]\n",
        "    rh_left = landmarks[right_indices[8]]\n",
        "    # vertical line \n",
        "    rv_top = landmarks[right_indices[12]]\n",
        "    rv_bottom = landmarks[right_indices[4]]\n",
        "     \n",
        "    # LEFT_EYE \n",
        "    # horizontal line \n",
        "    lh_right = landmarks[left_indices[0]]\n",
        "    lh_left = landmarks[left_indices[8]]\n",
        "\n",
        "    # vertical line \n",
        "    lv_top = landmarks[left_indices[12]]\n",
        "    lv_bottom = landmarks[left_indices[4]]\n",
        "\n",
        "    rhDistance = euclaideanDistance(rh_right, rh_left)\n",
        "    rvDistance = euclaideanDistance(rv_top, rv_bottom)\n",
        "\n",
        "    lvDistance = euclaideanDistance(lv_top, lv_bottom)\n",
        "    lhDistance = euclaideanDistance(lh_right, lh_left)\n",
        "\n",
        "    reRatio = rhDistance/rvDistance\n",
        "    leRatio = lhDistance/lvDistance\n",
        "\n",
        "    ratio = (reRatio+leRatio)/2\n",
        "    return ratio \n",
        "\n",
        "\n",
        "\n",
        "with map_face_mesh.FaceMesh(min_detection_confidence =0.5, min_tracking_confidence=0.5) as face_mesh:\n",
        "\n",
        "    # starting time here \n",
        "    start_time = time.time()\n",
        "    # starting Video loop here.\n",
        "    while True:\n",
        "        frame_counter +=1 # frame counter\n",
        "        ret, frame = camera.read() # getting frame from camera \n",
        "        if not ret: \n",
        "            break # no more frames break\n",
        "        #  resizing frame\n",
        "        \n",
        "        frame = cv.resize(frame, None, fx=1.5, fy=1.5, interpolation=cv.INTER_CUBIC)\n",
        "        frame_height, frame_width= frame.shape[:2]\n",
        "        rgb_frame = cv.cvtColor(frame, cv.COLOR_RGB2BGR)\n",
        "        results  = face_mesh.process(rgb_frame)\n",
        "        if results.multi_face_landmarks:\n",
        "            mesh_coords = landmarksDetection(frame, results, False)\n",
        "            ratio = blinkRatio(frame, mesh_coords, RIGHT_EYE, LEFT_EYE)\n",
        "            ratio_EYEBROW = \n",
        "            # cv.putText(frame, f'ratio {ratio}', (100, 100), FONTS, 1.0, GREEN, 2)\n",
        "            colorBackgroundText(frame,  f'Ratio : {round(ratio,2)}', FONTS, 0.7, (30,100),2, PINK, YELLOW)\n",
        "\n",
        "            if ratio >2.4:\n",
        "                CEF_COUNTER +=1\n",
        "                # cv.putText(frame, 'Blink', (200, 50), FONTS, 1.3, PINK, 2)\n",
        "                colorBackgroundText(frame,  f'Blink', FONTS, 1.7, (int(frame_height/2), 100), 2, YELLOW, pad_x=6, pad_y=6, )\n",
        "\n",
        "            else:\n",
        "                if CEF_COUNTER>CLOSED_EYES_FRAME:\n",
        "                    TOTAL_BLINKS +=1\n",
        "                    CEF_COUNTER =0\n",
        "            # cv.putText(frame, f'Total Blinks: {TOTAL_BLINKS}', (100, 150), FONTS, 0.6, GREEN, 2)\n",
        "            colorBackgroundText(frame,  f'Total Blinks: {TOTAL_BLINKS}', FONTS, 0.7, (30,150),2)\n",
        "            \n",
        "            cv.polylines(frame,  [np.array([mesh_coords[p] for p in LEFT_EYE ], dtype=np.int32)], True, GREEN, 1, cv.LINE_AA)\n",
        "            cv.polylines(frame,  [np.array([mesh_coords[p] for p in RIGHT_EYE ], dtype=np.int32)], True, GREEN, 1, cv.LINE_AA)\n",
        "\n",
        "\n",
        "\n",
        "        # calculating  frame per seconds FPS\n",
        "        end_time = time.time()-start_time\n",
        "        fps = frame_counter/end_time\n",
        "\n",
        "        frame =textWithBackground(frame,f'FPS: {round(fps,1)}',FONTS, 1.0, (30, 50), bgOpacity=0.9, textThickness=2)\n",
        "        # writing image for thumbnail drawing shape\n",
        "        # cv.imwrite(f'img/frame_{frame_counter}.png', frame)\n",
        "        cv2_imshow(frame)\n",
        "        key = cv.waitKey(2)\n",
        "        if key==ord('q') or key ==ord('Q'):\n",
        "            break\n",
        "    cv.destroyAllWindows()\n",
        "    camera.release()\n",
        " \n",
        "plt.plot(frame_counter, TOTAL_BLINKS)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IHSlvOtnh5V"
      },
      "source": [
        "Head Pose Estimation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s14MxNUOar8i",
        "outputId": "f1016553-d5b6-41e3-cd91-d239a64afe68"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
        "\n",
        "\n",
        "cap = cv2.VideoCapture('/content/production ID_3762907.mp4')\n",
        "\n",
        "while cap.isOpened():\n",
        "    success, image = cap.read()\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # Flip the image horizontally for a later selfie-view display\n",
        "    # Also convert the color space from BGR to RGB\n",
        "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # To improve performance\n",
        "    image.flags.writeable = False\n",
        "    \n",
        "    # Get the result\n",
        "    results = face_mesh.process(image)\n",
        "    \n",
        "    # To improve performance\n",
        "    image.flags.writeable = True\n",
        "    \n",
        "    # Convert the color space from RGB to BGR\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    img_h, img_w, img_c = image.shape\n",
        "    face_3d = []\n",
        "    face_2d = []\n",
        "\n",
        "    if results.multi_face_landmarks:\n",
        "        for face_landmarks in results.multi_face_landmarks:\n",
        "            for idx, lm in enumerate(face_landmarks.landmark):\n",
        "                if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
        "                    if idx == 1:\n",
        "                        nose_2d = (lm.x * img_w, lm.y * img_h)\n",
        "                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 3000)\n",
        "\n",
        "                    x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
        "\n",
        "                    # Get the 2D Coordinates\n",
        "                    face_2d.append([x, y])\n",
        "\n",
        "                    # Get the 3D Coordinates\n",
        "                    face_3d.append([x, y, lm.z])       \n",
        "            \n",
        "            # Convert it to the NumPy array\n",
        "            face_2d = np.array(face_2d, dtype=np.float64)\n",
        "\n",
        "            # Convert it to the NumPy array\n",
        "            face_3d = np.array(face_3d, dtype=np.float64)\n",
        "\n",
        "            # The camera matrix\n",
        "            focal_length = 1 * img_w\n",
        "\n",
        "            cam_matrix = np.array([ [focal_length, 0, img_h / 2],\n",
        "                                    [0, focal_length, img_w / 2],\n",
        "                                    [0, 0, 1]])\n",
        "\n",
        "            # The distortion parameters\n",
        "            dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
        "\n",
        "            # Solve PnP\n",
        "            success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
        "\n",
        "            # Get rotational matrix\n",
        "            rmat, jac = cv2.Rodrigues(rot_vec)\n",
        "\n",
        "            # Get angles\n",
        "            angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)\n",
        "\n",
        "            # Get the y rotation degree\n",
        "            x = angles[0] * 360\n",
        "            y = angles[1] * 360\n",
        "            z = angles[2] * 360\n",
        "          \n",
        "\n",
        "            # See where the user's head tilting\n",
        "            if y < -10:\n",
        "                text = \"Looking Left\"\n",
        "            elif y > 10:\n",
        "                text = \"Looking Right\"\n",
        "            elif x < -10:\n",
        "                text = \"Looking Down\"\n",
        "            elif x > 10:\n",
        "                text = \"Looking Up\"\n",
        "            else:\n",
        "                text = \"Forward\"\n",
        "\n",
        "            # Display the nose direction\n",
        "            nose_3d_projection, jacobian = cv2.projectPoints(nose_3d, rot_vec, trans_vec, cam_matrix, dist_matrix)\n",
        "\n",
        "            p1 = (int(nose_2d[0]), int(nose_2d[1]))\n",
        "            p2 = (int(nose_2d[0] + y * 10) , int(nose_2d[1] - x * 10))\n",
        "            \n",
        "            cv2.line(image, p1, p2, (255, 0, 0), 3)\n",
        "\n",
        "            # Add the text on the image\n",
        "            cv2.putText(image, text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 2)\n",
        "            cv2.putText(image, \"x: \" + str(np.round(x,2)), (500, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "            cv2.putText(image, \"y: \" + str(np.round(y,2)), (500, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "            cv2.putText(image, \"z: \" + str(np.round(z,2)), (500, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "\n",
        "        end = time.time()\n",
        "        totalTime = end - start\n",
        "\n",
        "        fps = 1 / totalTime\n",
        "        #print(\"FPS: \", fps)\n",
        "\n",
        "        cv2.putText(image, f'FPS: {int(fps)}', (20,450), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n",
        "\n",
        "        mp_drawing.draw_landmarks(\n",
        "                    image=image,\n",
        "                    landmark_list=face_landmarks,\n",
        "                    connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
        "                    landmark_drawing_spec=drawing_spec,\n",
        "                    connection_drawing_spec=drawing_spec)\n",
        "\n",
        "\n",
        "    cv2_imshow(image)\n",
        "\n",
        "\n",
        "    if cv2.waitKey(5) & 0xFF == 27:\n",
        "        break\n",
        "\n",
        "\n",
        "cap.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlA5FzLiyj3P"
      },
      "source": [
        " \n",
        "hand pose detector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6Pet6iCox2_d",
        "outputId": "11dbd0a4-a753-4ecc-fe60-f4787cdf373d"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "mp_pose = mp.solutions.pose\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# For webcam input:\n",
        "\n",
        "cap = cv2.VideoCapture('/content/hand_pose.mp4')\n",
        "with mp_pose.Pose(\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5) as pose:\n",
        "  while cap.isOpened():\n",
        "    success, image = cap.read()\n",
        "    if not success:\n",
        "      print(\"Ignoring empty camera frame.\")\n",
        "      # If loading a video, use 'break' instead of 'continue'.\n",
        "      continue\n",
        "\n",
        "    # To improve performance, optionally mark the image as not writeable to\n",
        "    # pass by reference.\n",
        "    image.flags.writeable = False\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = pose.process(image)\n",
        "\n",
        "    # Draw the pose annotation on the image.\n",
        "    image.flags.writeable = True\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    mp_drawing.draw_landmarks(\n",
        "        image,\n",
        "        results.pose_landmarks,\n",
        "        mp_pose.POSE_CONNECTIONS,\n",
        "        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
        "    # Flip the image horizontally for a selfie-view display.\n",
        "    cv2_imshow( cv2.flip(image, 1))\n",
        "    if cv2.waitKey(5) & 0xFF == 27:\n",
        "      break\n",
        "cap.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhKNR9QvnYuW"
      },
      "source": [
        "eyebrow with stress detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgOYfh2fjtmF"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial import distance as dist\n",
        "from imutils.video import VideoStream\n",
        "from imutils import face_utils\n",
        "import numpy as np\n",
        "import imutils\n",
        "import time\n",
        "import dlib\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "# from keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "from keras.models import load_model\n",
        "\n",
        "def eye_brow_distance(leye,reye):\n",
        "    global points\n",
        "    distq = dist.euclidean(leye,reye)\n",
        "    points.append(int(distq))\n",
        "    return distq\n",
        "\n",
        "def emotion_finder(faces,frame):\n",
        "    global emotion_classifier\n",
        "    EMOTIONS = [\"angry\" ,\"disgust\",\"scared\", \"happy\", \"sad\", \"surprised\",\"neutral\"]\n",
        "    x,y,w,h = face_utils.rect_to_bb(faces)\n",
        "    frame = frame[y:y+h,x:x+w]\n",
        "    roi = cv2.resize(frame,(64,64))\n",
        "    roi = roi.astype(\"float\") / 255.0\n",
        "    roi = img_to_array(roi)\n",
        "    roi = np.expand_dims(roi,axis=0)\n",
        "    preds = emotion_classifier.predict(roi)[0]\n",
        "    emotion_probability = np.max(preds)\n",
        "    label = EMOTIONS[preds.argmax()]\n",
        "    if label in ['scared','sad']:\n",
        "        label = 'stressed'\n",
        "    else:\n",
        "        label = 'not stressed'\n",
        "    return label\n",
        "    \n",
        "def normalize_values(points,disp):\n",
        "    normalized_value = abs(disp - np.min(points))/abs(np.max(points) - np.min(points))\n",
        "    stress_value = np.exp(-(normalized_value))\n",
        "    print(stress_value)\n",
        "    if stress_value>=75:\n",
        "        return stress_value,\"High Stress\"\n",
        "    else:\n",
        "        return stress_value,\"low_stress\"\n",
        "    \n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(\"/content/shape_predictor_68_face_landmarks.dat\")\n",
        "emotion_classifier = load_model(\"/content/_mini_XCEPTION.102-0.66.hdf5\", compile=False)\n",
        "cap = cv2.VideoCapture(0)\n",
        "points = []\n",
        "while(True):\n",
        "    _,frame = cap.read()\n",
        "    frame = cv2.flip(frame,1)\n",
        "    frame = imutils.resize(frame, width=500,height=500)\n",
        "    \n",
        "    \n",
        "    (lBegin, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eyebrow\"]\n",
        "    (rBegin, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eyebrow\"]\n",
        "\n",
        "    #preprocessing the image\n",
        "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    detections = detector(gray,0)\n",
        "    for detection in detections:\n",
        "        emotion = emotion_finder(detection,gray)\n",
        "        cv2.putText(frame, emotion, (10,10),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "        shape = predictor(frame,detection)\n",
        "        shape = face_utils.shape_to_np(shape)\n",
        "           \n",
        "        leyebrow = shape[lBegin:lEnd]\n",
        "        reyebrow = shape[rBegin:rEnd]\n",
        "            \n",
        "        reyebrowhull = cv2.convexHull(reyebrow)\n",
        "        leyebrowhull = cv2.convexHull(leyebrow)\n",
        "\n",
        "        cv2.drawContours(frame, [reyebrowhull], -1, (0, 255, 0), 1)\n",
        "        cv2.drawContours(frame, [leyebrowhull], -1, (0, 255, 0), 1)\n",
        "\n",
        "        distq = eye_brow_distance(leyebrow[-1],reyebrow[0])\n",
        "        stress_value,stress_label = normalize_values(points,distq)\n",
        "        cv2.putText(frame,\"stress level:{}\".format(str(int(stress_value*100))),(20,40),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    cv2_imshow(frame)\n",
        "    \n",
        "\n",
        "    key = cv2.waitKey(1) & 0xFF\n",
        "    if key == ord('q'):\n",
        "        break\n",
        "cv2.destroyAllWindows()\n",
        "cap.release()\n",
        "plt.plot(range(len(points)),points,'ro')\n",
        "plt.title(\"Stress Levels\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IU6dtY9MkFMA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
